
## ğŸš€ Project Overview

This application transforms static study materials into an **interactive AI-powered knowledge base**.

Users can:

* Upload one or more PDFs
* Ask questions in plain English
* Receive accurate answers generated by a local LLM
* See exactly **which document and chunk** the answer came from

All data, embeddings, and inference remain **private and offline**.

---

## ğŸ§  Key Features

* **PDF ingestion & text extraction**

  * Upload any PDF from the frontend
  * Text extracted using PyPDF2
  * Documents split into overlapping chunks for better context

* **Semantic search with ChromaDB**

  * SentenceTransformer embeddings (`all-MiniLM-L6-v2`)
  * Meaning-based retrieval instead of keyword search
  * Local persistent vector store

* **Local LLM Question Answering**

  * Powered by Ollama
  * Default model: `llama3.2:1b`
  * Automatic fallback to smaller models if needed
  * No external APIs or internet required

* **Modern Single-Page Frontend**

  * Built using HTML, CSS, and vanilla JavaScript
  * Glassmorphism UI with live system stats
  * Chat-style Q&A with source citations

* **Privacy-First & Offline**

  * No cloud services
  * No data leakage
  * Zero API cost

---

## ğŸ—ï¸ System Architecture (High Level)

```
PDF Upload
    â†“
Text Extraction (PyPDF2)
    â†“
Chunking + Overlap
    â†“
Embeddings (SentenceTransformer)
    â†“
ChromaDB (Vector Store)
    â†“
Relevant Context Retrieval
    â†“
Ollama (Local LLM)
    â†“
Answer + Source References
```

---

## ğŸ§° Tech Stack

**Backend**

* FastAPI
* ChromaDB
* SentenceTransformers
* PyPDF2
* Requests
* Uvicorn

**Frontend**

* HTML
* CSS
* Vanilla JavaScript

**LLM Runtime**

* Ollama (local models)

---

## ğŸ“‚ Project Structure

```
AI-Study-Assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py              # FastAPI backend (RAG + Ollama)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # App layout
â”‚   â”œâ”€â”€ styles.css           # UI styling
â”‚   â””â”€â”€ app.js               # Frontend logic
â”‚
â”œâ”€â”€ SETUP.md                 # Step-by-step execution guide
â”œâ”€â”€ TROUBLESHOOTING.md       # Ollama & local environment fixes
â””â”€â”€ README.md                # Project overview (this file)
```

---

## â–¶ï¸ Getting Started

To run the project locally, follow the detailed setup guide:

ğŸ“„ **[SETUP.md](./SETUP.md)** â€” complete installation & execution steps
ğŸ›  **[TROUBLESHOOTING.md](./TROUBLESHOOTING.md)** â€” common Ollama & runtime issues

---

## ğŸ¯ What This Project Demonstrates

* Retrieval-Augmented Generation (RAG)
* Vector databases & semantic search
* Local LLM orchestration with fallback
* Full-stack AI application design
* Clean project structure & documentation
  
---

## ğŸ”® Future Enhancements

* Streaming responses from Ollama
* Highlighting cited text in PDFs
* Per-document or per-session filtering
* Persistent chat history
* User authentication
* Dark / light mode toggle

---

## ğŸ™Œ Acknowledgements

* Ollama for local LLM runtime
* SentenceTransformers for embeddings
* ChromaDB for vector search
* FastAPI for backend framework

---

### â­ If you find this project useful, consider giving it a star!

---

