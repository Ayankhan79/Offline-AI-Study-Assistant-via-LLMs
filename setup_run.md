> ‚ö†Ô∏è **Important note**:
> `python main.py` will **NOT** start FastAPI unless your `main.py` explicitly runs `uvicorn`.
> FastAPI apps are started via **uvicorn**.

---

## ‚ñ∂Ô∏è How to Run the Project (3 Terminals)

### üñ•Ô∏è Terminal 1 ‚Äî Start Ollama (LLM Server)

```bash
ollama serve
```

Make sure at least one model is installed:

```bash
ollama pull llama3.2:1b
# or
ollama pull tinyllama
```

Leave this terminal running.

---

### üñ•Ô∏è Terminal 2 ‚Äî Start Backend (FastAPI)

```bash
cd backend
python -m venv .venv
.venv\Scripts\activate     # Windows
pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

Backend will be available at:

```
http://localhost:8000
```

---

### üñ•Ô∏è Terminal 3 ‚Äî Start Frontend

```bash
cd frontend
python -m http.server 3000
```

Open in browser:

```
http://localhost:3000/index.html
```

---

## ‚úÖ Corrected vs Your Original Commands

| Your command                 | Status            |
| ---------------------------- | ----------------- |
| `ollama serve`               | ‚úÖ Correct         |
| `python main.py`             | ‚ùå Needs `uvicorn` |
| `python -m http.server 3000` | ‚úÖ Correct         |

---

## üí° Optional (If you REALLY want `python main.py`)

If you *insist* on:

```bash
python main.py
```

Then add this at the **bottom of `main.py`**:

```python
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
```

After that, this will work:

```bash
cd backend
python main.py
```

---



